{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain-specific area \n",
    "\n",
    "The challenge which this peice of work will be addressing is the categorisation of the text of various\n",
    "different film reviews into categories of postitive or negative scores. This project will involve creating a binary classifier to pretict which category the reviews fall into.\n",
    "\n",
    "There is a very strong monetary incentive for people who create and fund films to care about sentiment classification of the reviews. The film industry is huge globally, having topped $100 billion in 2019 (Escandon, 2020) [1]. It has been found that reviews of films can have a real impact on their performance with significant impact on box office takings (Eagon, 2018)[2]. Correct classification of reviews written and posted online or collected during focus groups could give film-makers vital information to understand which films are liked by audiences and hence become more successful.\n",
    "    \n",
    "Furthermore, one type of film review which has become particularly prevelant in recent years is called 'review bombing'. This is where groups of people decide against a film and purposefully target it with lots of negative reviews [3]. Good text classification on film reviews could also help focus in on the negative reviews to spot 'review' bombing by picking up on a flurry of negative activity.\n",
    "    \n",
    "Additionally, by looking at the different features which most strongly predict positive and negative reviews, \n",
    "Film-makers can understand themes and ideas which are more likely to lead to different sentiments and keep them in mind\n",
    "when they are creating the next blockbuster smash hit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives \n",
    "\n",
    "The output of the project will be a Naive Bayes classifier which can be used to classify film reviews to ascertain if they are positive or negative - as determied by the content of the review. This project aims to find a Naive Bayes Classifier which exceeds the performance of a basic NB classifier which has been created without stop word removal using simple bag of word feautures.\n",
    "\n",
    "The data will be lemmatised and tokenised and it will be determined if stop word removal can improve the model performance. Stop word removal has been found to have potential to improve the performance of text classifiers (Silvia, 2003) [4].\n",
    "\n",
    "Then, the model will be further altered by instead using TF-IDF (combines term frequency and inverse document frequency [5])\n",
    "to see if a better model can be created. This may lead to a better model because Tf-Idf makes rare words more prominent and effectively ignores common words (M. Jain, 2021) [6]\n",
    "\n",
    "It will then be determined if the use of bi-grams and tri-grams can further improve the model. N-grams are used to show frequency of words appearing together in text:  N-grams analyses are often used to see which words often show up together (Yang, 2020) [7]. Collocations is the name for words that stick together more than would happen purely by chance.\n",
    "\n",
    "The results would help to determine the best model type for film review classification: with stop words removed or not, TF-IDF or Bag of words and solitary words or also including bi-grams and tri-grams.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "\n",
    "The dataset chosen for this text classification project is the imdb film review dataset from Stanford University (Maas, 2011) [8]. This dataset contains 50,000 film reviews which are either negative or positive. A negative review is one with a score of less than or equal to 4 and a postive review is one with a ascore of greater than or equal to 7 (with all scores given out of 10). There are no neutral scores included. \n",
    "\n",
    "The data set is pre-divided into test and train data with half of the data split as test and half as train. No more than 30 reviews for any one film are included. The data is hosted on a site by Stanford university and will be downloaded from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz using a script. \n",
    "\n",
    "The dataset, once downloaded, consists of a zipped folder with test and train folders each containing negative and positive folders, within these the reviews are saved each in seperate files There are two top-level directories. This will mean these data set will first need to be unzipped and then each review looped through and extracted to create a data set to work with in Python. \n",
    "\n",
    "The full size of these dataset may be too large to use without causing the code to be slow to run so a random sample may need to be taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation methodology \n",
    "\n",
    "Accuracy, Precision and Recall will be used to evaluate the Naive Bayes classifier created in this project and select the best one.\n",
    "\n",
    "The calculation for accuracy is the True Positives (TP) plus the True Negatives (TN) divided by the False Positives (FP), False Negatives (FN), True Positives and True Negatives -. In equation form: (TP + TN) / (TP + TN + FP + FN). \n",
    "This shows the percentages of classifications which were correct - it's not always a perfect measurement of classifier performance as if the data is imbalanced a classifier could score a high accuracy simply by always predicting the majority class - it will work for this scenario because it is known that the 2 different classes are equally balanced. \n",
    "\n",
    "Precision and Recall will also be used to add additional context. \n",
    "The calculation for precision is (TP/TP+FP) - this is the chance the that the class was correct given that it was chosen.\n",
    "The calculation for recall is (TP/TP+FN) - this is the chance, given the correct class, that it was chosen correctly.\n",
    "\n",
    "An F-measure could also be used which is a combination of precision and recall - however as this is primarily a useful addition for unbalanced datasets it will not be used to evaluate the model created in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: sklearn in c:\\programdata\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.24.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (1.19.2)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2020.10.15)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.50.2)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from sklearn) (0.23.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Fiona\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#Below are the requirements for the code to run - there is also a requirements file in the same folder as this code.\n",
    "\n",
    "!pip install nltk sklearn requests numpy pandas\n",
    "\n",
    "import requests\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.metrics import precision\n",
    "from nltk.metrics import recall\n",
    "import collections\n",
    "import random\n",
    "import sklearn\n",
    "nltk.download('stopwords')\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from itertools import filterfalse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquiring the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The below code pulls the film review data from the standford site and saves it in a folder named 'imdb_raw_data'\n",
    "#The file is zipped when it is downloaded and needs to be unzipped using tarfile\n",
    "\n",
    "\n",
    "saved_folder = \"imdb_raw_data\"  \n",
    "url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "\n",
    "response = requests.get(url)\n",
    "with open(\"aclImdb_v1.tar.gz\", 'wb') as f:\n",
    "    f.write(response.content)\n",
    "tar = tarfile.open(\"aclImdb_v1.tar.gz\", \"r:gz\")\n",
    "tar.extractall(saved_folder)\n",
    "tar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The names of the folders determine if the review is positive or negative\n",
    "#The below function uses this to add a binary flag each review\n",
    "\n",
    "def sentiment_from_path(path):\n",
    "    '''uses the filename to determine if the review is positive or negative'''\n",
    "    if re.match(\".*neg\", path):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "#Because the reviews are in seperate files they need to be pulled together \n",
    "#into one list of reviews paired with senitment scores\n",
    "\n",
    "def getting_raw_data(path):\n",
    "    '''for all individual review files, pulls them together to output list of reviews and sentiment'''\n",
    "    raw_data = []\n",
    "    sentiment = []\n",
    "    for filename in os.listdir(path):\n",
    "        open_data = open(os.path.join(path, filename), \"r\", encoding=\"utf-8\")\n",
    "        data_read = open_data.read()\n",
    "        sentiment.append(sentiment_from_path(path))\n",
    "        raw_data.append(data_read)\n",
    "    return raw_data, sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The file paths are created and fed into the function to get the data.\n",
    "#The actual path here as / is a special character in python so the seperate sections are joined using os.path.join\n",
    "\n",
    "test_negative = os.path.join(saved_folder, 'aclImdb', 'test', 'neg')\n",
    "test_positive = os.path.join(saved_folder, 'aclImdb', 'test', 'pos')\n",
    "train_negative = os.path.join(saved_folder, 'aclImdb', 'train', 'neg')\n",
    "train_positive = os.path.join(saved_folder, 'aclImdb', 'train', 'pos')\n",
    "\n",
    "test_n_data_all, test_n_Y_all = getting_raw_data(test_negative)\n",
    "test_p_data_all, test_p_Y_all = getting_raw_data(test_positive)\n",
    "train_n_data_all, train_n_Y_all = getting_raw_data(train_negative)\n",
    "train_p_data_all, train_p_Y_all = getting_raw_data(train_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For some sections of this project (particularly when removing stop-words) the code ran slowly\n",
    "#To improve the performance a random sample of 500 positives and 500 negatives have been taken for training and test each\n",
    "#A seed was used to make the random sample reproducable\n",
    "\n",
    "random.seed(500)\n",
    "test_n_data = random.sample(test_n_data_all, 500)\n",
    "test_n_Y = random.sample(test_n_Y_all, 500) \n",
    "test_p_data = random.sample(test_p_data_all, 500)\n",
    "test_p_Y = random.sample(test_p_Y_all, 500)\n",
    "train_n_data = random.sample(train_n_data_all, 500) \n",
    "train_n_Y = random.sample(train_n_Y_all, 500)\n",
    "train_p_data = random.sample(train_p_data_all, 500) \n",
    "train_p_Y = random.sample(train_p_Y_all, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning + Preprocessing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data is in English and contains informal reviews \n",
    "\n",
    "#The data now needs to be cleaned to remove spaces, line breaks from html and punctuation\n",
    "#The data also needs to be tokenised - dividing it up into component words \n",
    "#this is so the reviews can be classified by the words that they contain\n",
    "\n",
    "#The data is represented as 'Bag of Words'\n",
    "\n",
    "#The data also needs to be lemmatised - split down to their linguistic root\n",
    "#this is so that the classifier can spot when roots of words are frequently occuring and people may be saying the same things\n",
    "\n",
    "#The below function cleans, tokenises and lemmatises a review\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def tokenising_letimizing(review):\n",
    "    '''cleans, tokenises and lemmatises data'''\n",
    "    removed_spaces = re.sub(\"/\\s{1,}/g\",\" \", review)\n",
    "    removed_line_breaks = re.sub(\"<br />\", \"\" , removed_spaces)\n",
    "    removed_punctuation = re.sub(\"[^-9A-Za-z ]\", \"\" , removed_line_breaks)\n",
    "    tokenised = word_tokenize(removed_punctuation)\n",
    "    review_lem = []\n",
    "    for word in tokenised:\n",
    "        review_lem.append(lemmatizer.lemmatize(word))\n",
    "    return review_lem\n",
    "\n",
    "#This function loops through each review and applies the previous function\n",
    "#they have been seperated due to the different requirements of the nltk and sk_learn Naive Bayes fuctions\n",
    "\n",
    "def full_preprocessing(data):\n",
    "    '''loops through reviews of data to create list of them processed'''\n",
    "    data_processed = []\n",
    "    for review in data: \n",
    "        tl_review = tokenising_letimizing(review)\n",
    "        data_processed.append(tl_review)\n",
    "    return data_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the pre-processing is applied to the datasets\n",
    "\n",
    "test_n_cleaned = full_preprocessing(test_n_data)\n",
    "test_p_cleaned = full_preprocessing(test_p_data)\n",
    "train_n_cleaned = full_preprocessing(train_n_data)\n",
    "train_p_cleaned = full_preprocessing(train_p_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline performance \n",
    "\n",
    "The baseline against which the performance of the final classifier will be compared will be created by Naive Bayes using a simple BoW method with the existance within a review of certain (highg frequency) words being taken as features.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The positives and negatives are combined for post training and test\n",
    "\n",
    "data_train = train_n_cleaned + train_p_cleaned\n",
    "class_train = train_n_Y + train_p_Y\n",
    "data_test = test_n_cleaned + test_p_cleaned\n",
    "class_test = test_n_Y + test_p_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting high frequency words and using as features for baseline model\n",
    "\n",
    "#Looping through to get a list of all words in all training reviews\n",
    "all_words = []\n",
    "for review in data_train:\n",
    "    for word in review:\n",
    "        all_words.append(word)   \n",
    "\n",
    "#setting number of highest frequency words to 100\n",
    "N = 100\n",
    "\n",
    "#getting frequency of words for all words in the training reviews\n",
    "freq_of_words = nltk.FreqDist(all_words)\n",
    "\n",
    "#getting the top 100 most common words from the training reviews\n",
    "word_features = list(freq_of_words)[:N]\n",
    "\n",
    "#The below function creates features for each review with a flag saying if the review contains each of the 100 words\n",
    "def bow_features(review): \n",
    "    '''takes in a review and returns the features where these are a binary flag for if the review contains high freq words'''\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in review)\n",
    "    return features\n",
    "\n",
    "#Applying the feature function to the test dataset\n",
    "features =  [bow_features(review) for review in data_test]\n",
    "features_and_class = list(zip(features, class_test))\n",
    "\n",
    "#Applying the feature function to the training dataset\n",
    "features_train =  [bow_features(review) for review in data_train]\n",
    "features_and_class_train  = list(zip(features_train, class_train ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive precision: 63.80%\n",
      "negitive precision: 66.00%\n",
      "positive recall: 65.24%\n",
      "negitive recall: 64.58%\n",
      "accuracy: 64.90%\n"
     ]
    }
   ],
   "source": [
    "#generating and testing the basic naive bayes to get accuracy, recall and precision scores\n",
    "NBclassifier_baseline = nltk.NaiveBayesClassifier.train(features_and_class)\n",
    "\n",
    "\n",
    "def nltk_scores(classifier, features_and_class):\n",
    "    '''returns accuracy, recall and precision for nltk classifier'''\n",
    "    accuracy = nltk.classify.accuracy(classifier, features_and_class)\n",
    "    set_from_classifier = collections.defaultdict(set)\n",
    "    set_actual = collections.defaultdict(set)\n",
    " \n",
    "    for i, (features, sentiment) in enumerate(features_and_class):\n",
    "        set_actual[sentiment].add(i)\n",
    "        from_classifier = classifier.classify(features)\n",
    "        set_from_classifier[from_classifier].add(i)\n",
    "    positive_precision =  precision(set_from_classifier[0], set_actual[0])\n",
    "    negitive_precision = precision(set_from_classifier[1], set_actual[1])\n",
    "    positive_recall = recall(set_from_classifier[0], set_actual[0])\n",
    "    negitive_recall = recall(set_from_classifier[1], set_actual[1])\n",
    "    print ('positive precision:', \"{:.2%}\".format(positive_precision ))\n",
    "    print ('negitive precision:', \"{:.2%}\".format(negitive_precision ))\n",
    "    print ('positive recall:', \"{:.2%}\".format(positive_recall))\n",
    "    print ('negitive recall:', \"{:.2%}\".format(negitive_recall))\n",
    "    print ('accuracy:', \"{:.2%}\".format(accuracy))\n",
    "    \n",
    "\n",
    "nltk_scores(NBclassifier_baseline, features_and_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The baseline can thus be taken as 64.9% accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification approach \n",
    "Firstly the top 100 non stop word words will be used in the classifier, then features of a combination of term-frequency and doccument frequency will be used and bi-grams and tri-grams will also be added to see if the accuracy is improved.\n",
    "\n",
    "The classes will be 0 or 1: 0 for negative and 1 for positive - there is not class for neautral as there are no neutral reviews in the data and no neutral flag in the training data set.\n",
    "\n",
    "Naïve Bayes has been selected as the classifier which will be used in this approach. Naive Bayes is a simple and fast classifier and this project aims to discover how accurate it can become using different feature methods. Naive Bayes works by considering the prior probability of the classification and updating the results depending on the evidence provided (by the features)\n",
    "\n",
    "This will be a form of supervised learning as training data, pre classified is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "           contains(bad) = True                0 : 1      =      2.5 : 1.0\n",
      "         contains(great) = True                1 : 0      =      2.2 : 1.0\n",
      "          contains(well) = True                1 : 0      =      1.6 : 1.0\n",
      "           contains(the) = False               1 : 0      =      1.6 : 1.0\n",
      "            contains(of) = False               1 : 0      =      1.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#From looking at the most informative feautures of the classifier - we can see that some stop words \n",
    "#(commonly used words unlikely to impact sentiment) occur. The next step in creating classifier will be removing stopwords\n",
    "\n",
    "NBclassifier_baseline.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding in a section to remove stopwords to the initial processing function\n",
    "def tokenising_letimizing_sw(review):\n",
    "    '''cleans, tokenises and lemmatises data with stopwords removed'''\n",
    "    removed_spaces = re.sub(\"/\\s{1,}/g\",\" \", review)\n",
    "    removed_line_breaks = re.sub(\"<br />\", \"\" , removed_spaces)\n",
    "    removed_punctuation = re.sub(\"[^-9A-Za-z ]\", \"\" , removed_line_breaks)\n",
    "    tokenised = word_tokenize(removed_punctuation)\n",
    "    review_lem = []\n",
    "    for word in tokenised:\n",
    "        review_lem.append(lemmatizer.lemmatize(word))\n",
    "    review_nonsw = list(filterfalse(set(stopwords.words()).__contains__, review_lem))\n",
    "    return review_nonsw\n",
    "\n",
    "\n",
    "#changing the preprocessing function to include the new processing function\n",
    "def full_preprocessing_sw(data):\n",
    "    '''loops through reviews of data to create list of them processed - using function with stopwords removed'''\n",
    "    data_processed = []\n",
    "    for review in data: \n",
    "        tl_review = tokenising_letimizing_sw(review)\n",
    "        data_processed.append(tl_review)\n",
    "    return data_processed\n",
    "\n",
    "#removing stopwords from the initial data sets as well as pre-processing them \n",
    "test_n_cleaned_sw = full_preprocessing_sw(test_n_data)\n",
    "test_p_cleaned_sw = full_preprocessing_sw(test_p_data)\n",
    "train_n_cleaned_sw = full_preprocessing_sw(train_n_data)\n",
    "train_p_cleaned_sw = full_preprocessing_sw(train_p_data)\n",
    "\n",
    "#adding the positive and negative data together to become the test and train data\n",
    "data_train_sw =  train_n_cleaned_sw + train_p_cleaned_sw\n",
    "data_test_sw =  test_n_cleaned_sw + test_p_cleaned_sw\n",
    "\n",
    "#creating a full list of all of the non-stopword words \n",
    "words_nonsw = []\n",
    "for review in data_train_sw:\n",
    "    for word in review:\n",
    "        words_nonsw.append(word)   \n",
    "        \n",
    "#getting the frequency distribution of the non-stop-word words\n",
    "words_nonsw_dist = nltk.FreqDist(words_nonsw)\n",
    "\n",
    "#selecting the top 100 most common non-stop-word words\n",
    "word_features_nonsw = list(words_nonsw_dist)[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same as the previous code to create features except this uses the non stop word features\n",
    "def bow_features_sw(review):\n",
    "    '''takes in a review, returns the features where these are a binary flag for if the review contains high freq (non stop word) words'''\n",
    "    features = {}\n",
    "    for word in word_features_nonsw:\n",
    "        features['contains({})'.format(word)] = (word in review)\n",
    "    return features\n",
    "\n",
    "features_sw = [bow_features_sw(review) for review in data_test_sw]\n",
    "features_and_class_sw = list(zip(features_sw, class_test))\n",
    "\n",
    "features_train_sw = [bow_features_sw(review) for review in data_train_sw]\n",
    "features_and_class_train_sw  = list(zip(features_sw, class_train ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive precision: 71.20%\n",
      "negitive precision: 71.60%\n",
      "positive recall: 71.49%\n",
      "negitive recall: 71.31%\n",
      "accuracy: 71.40%\n"
     ]
    }
   ],
   "source": [
    "#generating the classifier\n",
    "\n",
    "NBclassifier_sw = nltk.NaiveBayesClassifier.train(features_and_class_sw)\n",
    "nltk_scores(NBclassifier_sw, features_and_class_sw)\n",
    "#this has improved the classifier to an accuracy score of 0.716\n",
    "#however I think we can do better than that by introducing tf idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_train_tfidf = train_n_data + train_p_data\n",
    "data_test_tfidf = test_n_data + test_p_data\n",
    "class_train_tfidf = train_n_Y + train_p_Y\n",
    "class_test_tfidf = test_n_Y + test_p_Y\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "\n",
    "#creating the function to use term frequency combined with inverse doccument frequency for words as features\n",
    "#this Naive Bayes classifier uses the library sk_learn\n",
    "#the tokenizer created previously is used which also removes stop-words\n",
    "\n",
    "def NB_sk_learn_tfidf(ngrams, own_tokenizer, train_data, train_classes, test_data):\n",
    "    '''uses sk_learn to create a NB Classifier for the data'''\n",
    "    vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(use_idf=True,ngram_range=(1,ngrams), \n",
    "                                                                  tokenizer = own_tokenizer)\n",
    "    tfidf_features_train = vectorizer.fit_transform(train_data)\n",
    "    tfidf_features_test = vectorizer.transform(test_data)\n",
    "    NBclassifier_tfidf = naive_bayes_classifier.fit(tfidf_features_train, train_classes)\n",
    "    class_pred = NBclassifier_tfidf.predict(tfidf_features_test)\n",
    "    return(class_pred, NBclassifier_tfidf, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive precision: 83.78%\n",
      "negitive precision: 77.64%\n",
      "positive recall: 75.40%\n",
      "negitive recall: 85.40%\n",
      "accuracy: 80.40%\n"
     ]
    }
   ],
   "source": [
    "#using tf-idf and not including bi-grams or tri-grams the accuacy increases to 79.6%\n",
    "\n",
    "tfidf_pred, NBclassifier_tfidf, vectorizer = NB_sk_learn_tfidf(1, tokenising_letimizing_sw, data_train_tfidf, class_train_tfidf, data_test_tfidf)\n",
    "\n",
    "def sk_learn_scores(class_test_tfidf, tfidf_pred):\n",
    "    '''returns accuracy, recall and precision for sk_learn classifier'''\n",
    "    print(\"positive precision:\", \"{:.2%}\".format(metrics.precision_score(class_test_tfidf, tfidf_pred, pos_label = 1)))\n",
    "    print(\"negitive precision:\", \"{:.2%}\".format(metrics.precision_score(class_test_tfidf, tfidf_pred, pos_label = 0)))\n",
    "    print(\"positive recall:\", \"{:.2%}\".format(metrics.recall_score(class_test_tfidf, tfidf_pred, pos_label = 1)))\n",
    "    print(\"negitive recall:\", \"{:.2%}\".format(metrics.recall_score(class_test_tfidf, tfidf_pred, pos_label = 0)))\n",
    "    print(\"accuracy:\", \"{:.2%}\".format(metrics.accuracy_score(class_test_tfidf, tfidf_pred)))    \n",
    "\n",
    "sk_learn_scores(class_test_tfidf, tfidf_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive precision: 84.18%\n",
      "negitive precision: 78.53%\n",
      "positive recall: 76.60%\n",
      "negitive recall: 85.60%\n",
      "accuracy: 81.10%\n"
     ]
    }
   ],
   "source": [
    "#using tf-idf , bi-grams but not tri-grams the accuacy increases to 81.9%\n",
    "\n",
    "tfidf_pred_bigram, NBclassifier_tfidf_bigram, vectorizer_bigram = NB_sk_learn_tfidf(2, tokenising_letimizing_sw, data_train_tfidf, class_train_tfidf, data_test_tfidf)\n",
    "sk_learn_scores(class_test_tfidf, tfidf_pred_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive precision: 83.30%\n",
      "negitive precision: 79.17%\n",
      "positive recall: 77.80%\n",
      "negitive recall: 84.40%\n",
      "accuracy: 81.10%\n"
     ]
    }
   ],
   "source": [
    "#using tf-idf , bi-grams and tri-grams the accuacy dips slightly to to 81.6%\n",
    "\n",
    "tfidf_pred_trigram, NBclassifier_tfidf_trigram, vectorizer_trigram = NB_sk_learn_tfidf(3, tokenising_letimizing_sw, data_train_tfidf, class_train_tfidf, data_test_tfidf)\n",
    "sk_learn_scores(class_test_tfidf, tfidf_pred_trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation \n",
    "\n",
    "Using accuracy it can be seen that the tf-idf method of feature creation with bigrams included and with stop words removed  was the best classifier created in the above code. \n",
    "The results for the evaluation metrics are as follows:\n",
    "\n",
    "\n",
    "| Model                                   | Accuracy     |\n",
    "| --------------------------------------- | ------------ |\n",
    "| NBclassifier_baseline                   | 64.90%       |\n",
    "| NBclassifier_sw (no stop words)         | 71.60%       |\n",
    "| NBclassifier_tfidf                      | 79.60%       | \n",
    "| NBclassifier_tfidf_bigram               | 81.90%       | \n",
    "| NBclassifier_tfidf_trigram              | 81.60%       | \n",
    "\n",
    "The results for precision are as follows:\n",
    "\n",
    "| Model                                   | Precision +ve  |\n",
    "| --------------------------------------- | -------------- |\n",
    "| NBclassifier_baseline                   | 63.80%         |\n",
    "| NBclassifier_sw (no stop words)         | 66.20%         |\n",
    "| NBclassifier_tfidf                      | 87.76%         | \n",
    "| NBclassifier_tfidf_bigram               | 88.25%         | \n",
    "| NBclassifier_tfidf_trigram              | 87.26%         | \n",
    "\n",
    "| Model                                   | Precision -ve  |\n",
    "| --------------------------------------- | -------------- |\n",
    "| NBclassifier_baseline                   | 66.00%         |\n",
    "| NBclassifier_sw (no stop words)         | 77.00%         |\n",
    "| NBclassifier_tfidf                      | 74.34%         | \n",
    "| NBclassifier_tfidf_bigram               | 77.36%         | \n",
    "| NBclassifier_tfidf_trigram              | 77.43%         | \n",
    "\n",
    "The results for recall are as follows:\n",
    "\n",
    "| Model                                   | Recall +ve   |\n",
    "| --------------------------------------- | ------------ |\n",
    "| NBclassifier_baseline                   | 65.24%       |\n",
    "| NBclassifier_sw (no stop words)         | 74.22%       |\n",
    "| NBclassifier_tfidf                      | 68.80%       | \n",
    "| NBclassifier_tfidf_bigram               | 73.60%       | \n",
    "| NBclassifier_tfidf_trigram              | 74.00%       | \n",
    "\n",
    "| Model                                   | Recall -ve   |\n",
    "| --------------------------------------- | ------------ |\n",
    "| NBclassifier_baseline                   | 64.58%       |\n",
    "| NBclassifier_sw (no stop words)         | 69.49%       |\n",
    "| NBclassifier_tfidf                      | 90.40%       | \n",
    "| NBclassifier_tfidf_bigram               | 90.20%       | \n",
    "| NBclassifier_tfidf_trigram              | 89.20%       | \n",
    "\n",
    "For overall accuracyd the best Naive Bayes model is the one with tf-idf method of feature creation with bigrams included and with stop words removed (81.9% accuracy)\n",
    "There are no signs from recall and precision that the classifiers are over predicting one sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Conclusions\n",
    "\n",
    "In conclusion, it can be seen that removing stop words leads to a more accurate classifier of sentiment for film reviews. \n",
    "Adding in term-frequency combined with inverse doccument frequency leads to improved performance and the addition of bi-grams (pairs of words commonky found together) as features improves the model further still. \n",
    "\n",
    "The inclusion of tri-grams was not found to improve the model however the performance was still increased over the model which included neither bi-grams nor tri-grams. \n",
    "\n",
    "The solution which was created by this project may have some success for other film review areas such as tweets about a film however the type of langage used may be less formal in tweets and context may be lost (people may also post images as part of their twitter film commentary). It is also possible that sarcasm would be used on twitter which would be less likely to be seen on an imdb film review. \n",
    "\n",
    "It may  be difficult to use the classifier built on text other than text about films as there may be domain specific comments which would not translate - book reviews would not include comments about run time duration for example.\n",
    "\n",
    "This apprach could be replicated by any libraies which allow for Naive Bayes text classification with tf-idf features and n-grams. A suitable library for R would be naivebayes. \n",
    "\n",
    "A beneficial alternative approach could be to also try other types of classifier eg. Logistic Regression and compare them against each other with similar steps to the ones detailed above. A reason why the method in this project was chosen is that it is quick and simple - Niave Bayes an be trained very quickly compared to some other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] Eagon, O., 2018. The Influence of Film Critics on Movie Outcomes. [online] www.researchgate.net. Available at: <https://www.researchgate.net/publication/330015688_The_Influence_of_Film_Critics_on_Movie_Outcomes> [Accessed 30 December 2021].\n",
    "\n",
    "\n",
    "[2] Escandon, R., 2020. The Film Industry Made A Record-Breaking $100 Billion Last Year. [online] Forbes. Available at: <https://www.forbes.com/sites/rosaescandon/2020/03/12/the-film-industry-made-a-record-breaking-100-billion-last-year/?sh=585a03d34cd6> [Accessed 30 December 2021].\n",
    "\n",
    "\n",
    "[3] Wordsworth, R., 2019. The secrets of 'review-bombing': why do people write zero-star reviews?. [online] the Guardian. Available at: <https://www.theguardian.com/games/2019/mar/25/review-bombing-zero-star-reviews> [Accessed 30 December 2021].\n",
    "\n",
    "\n",
    "[4] C. Silva and B. Ribeiro, 2003,  \"The importance of stop word removal on recall values in text categorization\",\n",
    "Proceedings of the International Joint Conference on Neural Networks\n",
    "\n",
    "\n",
    "[5] C. D. Manning, P. Raghavan and H. Schütze, 2008, \"Introduction to Information Retrieval\", Cambridge University Press\n",
    "\n",
    "\n",
    "[6]Jain, M., 2021. Why Tf-Idf is more effective than Bag-Of-Words?. [online] Medium. Available at: <https://ai.plainenglish.io/why-tf-idf-is-more-effective-than-bag-of-words-49ba175247c3> [Accessed 1 January 2022].\n",
    "\n",
    "\n",
    "[7] Medium. 2020. Text analysis basics in Python. [online] Available at: <https://towardsdatascience.com/text-analysis-basics-in-python-443282942ec5> [Accessed 1 January 2022].\n",
    "\n",
    "\n",
    "[8] Maas, A. L., Daly, R. E., Pham, P. T., Huang, D, Ng, A. Y.  and  Potts, C., 2011, \"Learning Word Vectors for Sentiment Analysis\", Association for Computational Linguistics, pp. 142-150\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
